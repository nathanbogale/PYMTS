{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python311\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\python311\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\python311\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\python311\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python311\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\python311\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in c:\\python311\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\python311\\lib\\site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\python311\\lib\\site-packages (from seaborn) (2.0.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\python311\\lib\\site-packages (from seaborn) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python311\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: toad in c:\\python311\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\python311\\lib\\site-packages (from toad) (65.5.0)\n",
      "Requirement already satisfied: Cython>=0.29.15 in c:\\python311\\lib\\site-packages (from toad) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\python311\\lib\\site-packages (from toad) (1.24.3)\n",
      "Requirement already satisfied: pandas in c:\\python311\\lib\\site-packages (from toad) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\python311\\lib\\site-packages (from toad) (1.10.1)\n",
      "Requirement already satisfied: joblib>=0.12 in c:\\python311\\lib\\site-packages (from toad) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in c:\\python311\\lib\\site-packages (from toad) (1.2.2)\n",
      "Requirement already satisfied: seaborn>=0.10.0 in c:\\python311\\lib\\site-packages (from toad) (0.12.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python311\\lib\\site-packages (from scikit-learn>=0.21->toad) (3.1.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\python311\\lib\\site-packages (from seaborn>=0.10.0->toad) (3.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python311\\lib\\site-packages (from pandas->toad) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas->toad) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python311\\lib\\site-packages (from pandas->toad) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.10.0->toad) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.10.0->toad) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.10.0->toad) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.10.0->toad) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.10.0->toad) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.10.0->toad) (9.5.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.10.0->toad) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->toad) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\python311\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import pip\n",
    "installedPackages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "required = { 'pandas','numpy', 'matplotlib', 'seaborn','toad','scikit-learn','sklearn'}\n",
    "missing = required - installedPackages\n",
    "if missing:\n",
    "    !pip install pandas\n",
    "    !pip install numpy\n",
    "    !pip install matplotlib\n",
    "    !pip install seaborn\n",
    "    !pip install toad\n",
    "    !pip install scikit-learn\n",
    "    !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtoad\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score,roc_curve,auc,precision_recall_curve\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sklearn\n",
    "import toad\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc,precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, start, end, date_col):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df[date_col] >= start) & (df[date_col] < end)]\n",
    "    #data = df[(df.order_date >= start) & (df.order_date < end)]\n",
    "    data = data.reset_index(drop=True)\n",
    "    #data = data.sort_values([\"date\", \"tic\"], ignore_index=True)\n",
    "    #data.index = data.date.factorize()[0]\n",
    "    return data\n",
    "\n",
    "\n",
    "def target_info(df_target_column):\n",
    "    '''\n",
    "    Print target column information -- count for each unique values in target column\n",
    "    Input: target column in dataframe\n",
    "    '''\n",
    "    proportion_true=str(round(sum(df_target_column == True) / len(df_target_column), 2) * 100)\n",
    "    \n",
    "    proportion_false=str(round(sum(df_target_column == False) / len(df_target_column), 3) * 100)\n",
    "    \n",
    "    print('There are total {} records in our data.'.format(len(df_target_column)))\n",
    "    print('Is Fraud:')\n",
    "    print('Count: {}'.format(sum(df_target_column == True)))\n",
    "    print('Proportion (Fraud): {}'.format(proportion_true\n",
    "                                  + '%'))\n",
    "    print('Not Fraud:')\n",
    "    print('Count: {}'.format(sum(df_target_column == False)))\n",
    "    print('Proportion (Not Fraud): {}'.format(proportion_false\n",
    "                                  + '%'))\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.barplot(x=df_target_column.value_counts().index, y=df_target_column.value_counts())\n",
    "    plt.title('Not Fraud vs Fraud Counts')\n",
    "    plt.ylabel('Count')\n",
    "    return proportion_true\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to load the csv file\n",
    "data = pd.read_csv('UCI_Credit_Card.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the size of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check few lines\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the world 'label'\n",
    "data['label']=data['default.payment.next.month']\n",
    "data=data.drop(columns=['default.payment.next.month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the fraud proportion of the data\n",
    "target_info(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an exclude list for the scorecard package Toad\n",
    "exclude_list = ['ID','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ID.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the ID column to split the train-test data\n",
    "train = data_split(data,start = 0, end=22500,date_col='ID')\n",
    "test = data_split(data,start = 22500, end=172792,date_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###feature filtering by missing value, IV & corrrelation：\n",
    "##If the missing value rate is greater than the threshold, delete the feature\n",
    "##If the correlation coefficient is greater than the threshold, delete the feature\n",
    "##If the IV is smaller than the threshold, delete the features\n",
    "\n",
    "train_selected, drop_lst= toad.selection.select(frame = train,\n",
    "                                                target=train['label'], \n",
    "                                                empty = 0.7, \n",
    "                                                iv = 0.02, corr = 1, \n",
    "                                                return_drop=True, \n",
    "                                                exclude=exclude_list)\n",
    "print(\"keep:\",train_selected.shape[1],\n",
    "      \"drop empty:\",len(drop_lst['empty']),\n",
    "      \"drop iv:\",len(drop_lst['iv']),\n",
    "      \"drop corr:\",len(drop_lst['corr']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the iv table to a dataframe\n",
    "def output_iv_importance(train_selected,label_col):\n",
    "    feat_import_iv = toad.quality(train_selected,label_col,iv_only=True)\n",
    "    feat_import_iv=feat_import_iv['iv']\n",
    "    feat_import_iv = feat_import_iv.reset_index()\n",
    "    feat_import_iv.columns = ['name','iv']\n",
    "    return feat_import_iv\n",
    "df_iv=output_iv_importance(train_selected,'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Binning\n",
    "Feature binning is to transform a continuous or numerical variable into a categorical feature.\n",
    "* It simplifies the logistic regression model and reduces the risk of model overfitting\n",
    "* Logistic regression is a generalized linear model, and its expressive ability is limited; Feature binning can introduce nonlinearity into the model, which can improve the expressive ability of the model and help better model fitting\n",
    "* The discretized features are very robust to abnormal data: for example, a feature is 1 if age > 30, and 0 otherwise. If the features are not discretized, an abnormal data point \"300 years old\" will impact the model fitting\n",
    "* It can treat null data as an individual class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selected.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for feature binning:\n",
    "#### Step 1. Initialization: c = toad.transform.Combiner() \n",
    "#### Step 2. Training binning: \n",
    "c.fit(dataframe, \n",
    "      y = 'target', \n",
    "      method = 'chi', \n",
    "      min_samples = 0.05, \n",
    "      n_bins = None, \n",
    "      empty_separate = False)\n",
    "* y: target column\n",
    "* method: binning method, supports chi (chi-square binning), dt (decision tree binning), kmean, quantile, step (equal step size binning)\n",
    "* min_samples: Each box contains the least number of samples, which can be a number or a proportion \n",
    "* n_bins: the number of bins; If it is not possible to divide so many boxes the maximum number of bins will be divided.\n",
    "* empty_separate: Whether to separate empty boxes separately \n",
    "\n",
    "#### Step 3. check binning nodes: c.export() \n",
    "#### Step 4. Manually adjust binning: c.load(dict) \n",
    "#### Step 5. Apply binning results: c.transform(dataframe, labels=False)\n",
    "* labels: Whether to convert the binning results into box labels. If False, output 0, 1, 2… (discrete variables are sorted according to the proportion), and if True output (-inf, 0], (0,10], (10, inf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "combiner = toad.transform.Combiner()\n",
    "# use the filtered features for training\n",
    "# Use the stable chi-square binning, \n",
    "# specifying that each bin has at least 5% data to ensure stability\n",
    "# empty values will be automatically assigned to the best bin\n",
    "combiner.fit(X=train_selected,\n",
    "             y=train_selected['label'],\n",
    "             method='chi',\n",
    "             min_samples = 0.05,\n",
    "             exclude=exclude_list)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 1: pickle combiner\n",
    "filename = 'CreditScore_save1_combiner.pkl'\n",
    "pickle.dump(combiner, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "#combiner = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output binning\n",
    "bins = combiner.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply binning\n",
    "train_selected_bin = combiner.transform(train_selected)\n",
    "test_bin = combiner.transform(test[train_selected_bin.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selected_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [feat for feat in train_selected_bin.columns if feat not in exclude_list]\n",
    "len(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune bins\n",
    "from toad.plot import  bin_plot,badrate_plot\n",
    "bin_plot(train_selected_bin,x='PAY_AMT1',target='label')\n",
    "bin_plot(test_bin,x='PAY_AMT1',target='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting rules\n",
    "#rule = {'PAY_AMT1':[['0', 'nan'],['1'], ['2'], ['3']]}\n",
    "\n",
    "#Adjust binning\n",
    "#c.set_rules(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selected_bin.PAY_0.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transform to WOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##转化成WOE映射\n",
    "t=toad.transform.WOETransformer()\n",
    "#transform training set\n",
    "train_woe = t.fit_transform(X=train_selected_bin,\n",
    "                            y=train_selected_bin['label'], \n",
    "                            exclude=exclude_list)\n",
    "#transform testing set\n",
    "test_woe = t.transform(test_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_woe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_woe = pd.concat([train_woe,test_woe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 2: pickle woe_transform\n",
    "filename = 'CreditScore_save2_woe_transform.pkl'\n",
    "pickle.dump(t, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_use = [feat for feat in final_data_woe.columns if feat not in exclude_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate PSI\n",
    "* PSI (Population Stability Index) reflects the stability of the distribution. We often use it to screen features and evaluate model stability. The industry level is to drop features with a PSI greater than 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the feature name\n",
    "features_list = [feat for feat in train_woe.columns if feat not in exclude_list]\n",
    "#calculate PSI using toad\n",
    "psi_df = toad.metrics.PSI(train_woe[features_list], test_woe[features_list]).sort_values(0)\n",
    "#put into a dataframe\n",
    "psi_df = psi_df.reset_index()\n",
    "psi_df = psi_df.rename(columns = {'index' : 'feature',0:'psi'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features less than 0.25\n",
    "psi025 = list(psi_df[psi_df.psi<0.25].feature)\n",
    "# features geater than 0.25\n",
    "psi_remove = list(psi_df[psi_df.psi>=0.25].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep exclude list\n",
    "for i in exclude_list:\n",
    "    if i in psi025:\n",
    "        pass\n",
    "    else:\n",
    "       psi025.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features that are geater than 0.25\n",
    "train_selected_woe_psi = train_woe[psi025]\n",
    "off_woe_psi = test_woe[psi025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output our final data table\n",
    "final_data_woe = pd.concat([train_selected_woe_psi,off_woe_psi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_data_woe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 3: final data table with transformed woe\n",
    "final_data_woe.to_csv('CreditScore_save3_final_data_woe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Output IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_use = [feat for feat in final_data_woe.columns if feat not in exclude_list]\n",
    "len(features_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv=output_iv_importance(final_data_woe[features_use+['label']],'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 4: information value after woe transformation\n",
    "df_iv.to_csv('CreditScore_save4_IV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_test_auc(x_train,y_train,x_test,y_test):\n",
    "    # 用逻辑回归建模\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr = LogisticRegression(random_state=42,C= 0.1, penalty='l2', solver='newton-cg')\n",
    "\n",
    "\n",
    "    lr = LogisticRegression(class_weight='balanced')\n",
    "    lr.fit(x_train, y_train)\n",
    "\n",
    "    # 预测训练和隔月的OOT\n",
    "    pred_train = lr.predict_proba(x_train)[:,1]\n",
    "    from toad.metrics import KS, AUC\n",
    "\n",
    "    print('train KS',KS(pred_train, y_train))\n",
    "    print('train AUC',AUC(pred_train, y_train))\n",
    "    \n",
    "    pred_OOT =lr.predict_proba(x_test)[:,1]\n",
    "    print('Test KS',KS(pred_OOT, y_test))\n",
    "    print('Test AUC',AUC(pred_OOT, y_test))\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, plot_roc_curve, classification_report\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    plot_roc_curve(lr, x_test, y_test, color='blue', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train & test\n",
    "check_train_test_auc(x_train = train_woe[features_use],y_train=train_woe['label'],\n",
    "                     x_test =test_woe[features_use] ,y_test = test_woe['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_scores(label, predictions):\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    tp, fn, fp, tn = confusion_matrix(label,predictions,labels=[1,0]).reshape(-1)\n",
    "    print('True Positive：',tp)\n",
    "    print('True Negative：',tn)\n",
    "    print('False Positive：',fp)\n",
    "    print('False Negative：',fn)\n",
    "    accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "    print('accuracy: ',accuracy)\n",
    "    recall = tp/(tp+fn)\n",
    "    print('（recall）: ',recall)\n",
    "    precision = tp/(tp+fp)\n",
    "    print('（precision）: ',precision)\n",
    "    #f1 score = 2*(P*R)/(P+R)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    print('F1 score: ',f1)\n",
    "    \n",
    "    print(classification_report(label, predictions))\n",
    "    \n",
    "    print('balanced_accuracy_score: ',balanced_accuracy_score(label,predictions))\n",
    "    return precision, recall\n",
    "\n",
    "def evaluate_result(df_train,df_test,features_name):\n",
    "    from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    start = time.time()\n",
    "    x_train = df_train[features_name]\n",
    "    y_train = df_train['label']\n",
    "\n",
    "    x_test  = df_test[features_name]\n",
    "    y_test  = df_test['label']\n",
    "\n",
    "    model = GradientBoostingClassifier(n_estimators=250,random_state=0)\n",
    "    model.fit(x_train,y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    get_evaluation_scores(label = y_test, predictions=predictions)\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=features_name)\n",
    "    feat_importances=pd.DataFrame(feat_importances).reset_index()\n",
    "    feat_importances.columns=['feature_name','feature_importance']\n",
    "    feat_importances=feat_importances.sort_values(['feature_importance'],ascending=False)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(15,15))\n",
    "\n",
    "    sns_plot1=sns.barplot(feat_importances.feature_importance,feat_importances.feature_name,estimator=sum)\n",
    "    plt.title(\"Features Importance\",size=18)\n",
    "    plt.ylabel('', size = 15)\n",
    "    plt.tick_params(labelsize=18)\n",
    "    return feat_importances,model,x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a GBDT and check the feauture importance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fet_importance_GBDT_reason,model,x_train,y_train,x_test,y_test = evaluate_result(df_train=train_woe,\n",
    "                df_test=test_woe,\n",
    "                features_name=features_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fet_importance_GBDT_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_pre_recall_curve(labels, probs):\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    # Get ROC curve FPR and TPR from true labels vs score values\n",
    "    fpr, tpr, _ = roc_curve(labels, probs)\n",
    "\n",
    "    # Calculate ROC Area Under the Curve (AUC) from FPR and TPR data points\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Calculate precision and recall from true labels vs score values\n",
    "    precision, recall, _ = precision_recall_curve(labels, probs)\n",
    "\n",
    "    plt.figure(figsize=(8, 3))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.step(recall, precision, color='orange', where='post')\n",
    "    # plt.fill_between(recall, precision, step='post', alpha=0.5, color='orange')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.grid(True)\n",
    "\n",
    "    left  = 0.125  # the left side of the subplots of the figure\n",
    "    right = 0.9    # the right side of the subplots of the figure\n",
    "    bottom = 0.1   # the bottom of the subplots of the figure \n",
    "    top = 0.9      # the top of the subplots of the figure\n",
    "    wspace = 0.5   # the amount of width reserved for blank space between subplots\n",
    "    hspace = 0.2   # the amount of height reserved for white space between subplots\n",
    "    plt.subplots_adjust(left, bottom, right, top, wspace, hspace)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(x_test)[:,1]\n",
    "sns.set(font_scale = 1)\n",
    "plot_roc_pre_recall_curve(y_test, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare train & test data\n",
    "x_train = train_woe[features_use]\n",
    "y_train=train_woe['label']\n",
    "x_test =test_woe[features_use] \n",
    "y_test = test_woe['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train LR\n",
    "#lr = LogisticRegression(random_state=42,C= 0.1, penalty='l2', solver='newton-cg')\n",
    "lr = LogisticRegression(class_weight = 'balanced')\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check AUC\n",
    "probs = lr.predict_proba(x_test)[:,1]\n",
    "sns.set(font_scale = 1)\n",
    "plot_roc_pre_recall_curve(y_test, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Scorecard Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorecard tuning\n",
    "card = toad.ScoreCard(\n",
    "    combiner = combiner,\n",
    "    transer = t,\n",
    "    class_weight = 'balanced',\n",
    "    C=0.1,\n",
    "    base_score = 1000,\n",
    "    base_odds = 35 ,\n",
    "    pdo = 80,\n",
    "    rate = 2\n",
    ")\n",
    "\n",
    "card.fit(train_woe[features_use], train_woe['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 5: save the model to disk\n",
    "filename = 'CreditScore_save5_ScoreCard.pkl'\n",
    "pickle.dump(card, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference on test data\n",
    "test['CreditScore'] = card.predict(test)\n",
    "test['CreditScore'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference on whole data\n",
    "data['CreditScore'] = card.predict(data)\n",
    "data['CreditScore'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the scorecard\n",
    "final_card_score=card.export()\n",
    "len(final_card_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the scorecard into dataframe and save to csv\n",
    "keys = list(card.export().keys())\n",
    "score_card_df = pd.DataFrame()\n",
    "for n in keys:\n",
    "    temp = pd.DataFrame.from_dict(final_card_score[n], orient='index')\n",
    "    temp = temp.reset_index()\n",
    "    temp.columns= ['binning','score']\n",
    "    temp['variable'] = n\n",
    "    temp = temp[['variable','binning','score']]\n",
    "    score_card_df=score_card_df.append(temp)\n",
    "score_card_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 6: save the scorcard\n",
    "score_card_df.to_csv('CreditScore_save6_score_card_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "import random\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "w = 40\n",
    "n = math.ceil((data['CreditScore'].max() - data['CreditScore'].min())/w)\n",
    "#bins = numpy.linspace(-10, 10, 100)\n",
    "\n",
    "plt.hist(data[data.label==1].CreditScore, alpha=0.5, label='Black',bins = n)\n",
    "plt.hist(data[data.label==0].CreditScore, alpha=0.5, label='White',bins = n)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Credit Score Distribution: Test Set',size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "import random\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "w = 40\n",
    "n = math.ceil((test['CreditScore'].max() - test['CreditScore'].min())/w)\n",
    "#bins = numpy.linspace(-10, 10, 100)\n",
    "\n",
    "plt.hist(test[test.label==1].CreditScore, alpha=0.5, label='Black',bins = n)\n",
    "plt.hist(test[test.label==0].CreditScore, alpha=0.5, label='White',bins = n)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Credit Score Distribution: Whole Dataset',size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credit_level(\n",
    "    test,\n",
    "    target_score ='order_score',\n",
    "    out_col = 'order_level',\n",
    "    left_bound = -100,\n",
    "    level_0 = 100,\n",
    "    level_1 = 200,    \n",
    "    level_2 = 250,    \n",
    "    level_3 = 300,    \n",
    "    level_4 = 350,    \n",
    "    level_5 = 400,    \n",
    "    level_6 = 450,\n",
    "    level_7 = 500,\n",
    "    level_8 = 800):\n",
    "    level = []\n",
    "    for i in range(len(test)):\n",
    "        if (test[target_score][i]>left_bound) & (test[target_score][i]<=level_0):\n",
    "            level.append(0)\n",
    "        elif  (test[target_score][i]>level_0) & (test[target_score][i]<=level_1):\n",
    "            level.append(1)\n",
    "        elif  (test[target_score][i]>level_1) & (test[target_score][i]<=level_2):\n",
    "            level.append(2)\n",
    "        elif  (test[target_score][i]>level_2) & (test[target_score][i]<=level_3):\n",
    "            level.append(3)\n",
    "        elif  (test[target_score][i]>level_3) & (test[target_score][i]<=level_4):\n",
    "            level.append(4)\n",
    "        elif  (test[target_score][i]>level_4) & (test[target_score][i]<=level_5):\n",
    "            level.append(5)\n",
    "        elif  (test[target_score][i]>level_5) & (test[target_score][i]<=level_6):\n",
    "            level.append(6)\n",
    "        elif  (test[target_score][i]>level_6) & (test[target_score][i]<=level_7):\n",
    "            level.append(7)\n",
    "        elif  (test[target_score][i]>level_7 )& (test[target_score][i]<=level_8):\n",
    "            level.append(8)\n",
    "        \n",
    "    test[out_col] = level\n",
    "    return test\n",
    "\n",
    "def plot_bts_level_loss(test, target_col):\n",
    "    bts_level_df = test[target_col].value_counts()\n",
    "    bts_level_df=pd.DataFrame(bts_level_df)\n",
    "    df_label_level= test[test.label==1].groupby(target_col)['label'].count()/ test.groupby(target_col)['label'].count()\n",
    "    df_label_level = pd.DataFrame(df_label_level)\n",
    "    bts_level_df.sort_index().plot.bar(title='')\n",
    "    df_label_level.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.CreditScore.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Tuning & Trade-off between loss & Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_credit_level(test,\n",
    "                       target_score ='CreditScore',\n",
    "                       out_col = 'CreditScore_level',\n",
    "                      left_bound = -1000,\n",
    "    level_0 = 250,\n",
    "    level_1 = 300,    \n",
    "    level_2 = 400,    \n",
    "    level_3 = 500,    \n",
    "    level_4 = 580,    \n",
    "    level_5 = 630,    \n",
    "    level_6 = 690,\n",
    "    level_7 = 730,\n",
    "    level_8 = 1000\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bts_level_loss(test,target_col='CreditScore_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test[test.label==1].CreditScore_level==7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_coverage(test,target_level):\n",
    "    \n",
    "    #level 8 Loss\n",
    "    L8_loss=test[test[target_level]>=8 ].label.value_counts()/len(test[test[target_level]>=8 ])\n",
    "    #level 8 Coverage\n",
    "    L8_coverage=test[test[target_level]>=8].label.value_counts()[0]/test[test.label==0].shape[0]\n",
    "    print(\"Level 8: Loss is \",L8_loss[1], \"; Coverage is \",L8_coverage)\n",
    "    \n",
    "    #level 7-level 8 Loss\n",
    "    L7_loss=test[test[target_level]>=7 ].label.value_counts()/len(test[test[target_level]>=7 ])\n",
    "    #level 7-level 8 Coverage\n",
    "    L7_coverage=test[test[target_level]>=7].label.value_counts()[0]/test[test.label==0].shape[0]\n",
    "    print(\"Level 7-Level 8: Loss is \",L7_loss[1], \"; Coverage is \",L7_coverage)\n",
    "    \n",
    "    #level 6-level 8 Loss\n",
    "    L6_loss=test[test[target_level]>=6 ].label.value_counts()/len(test[test[target_level]>=6 ])\n",
    "    #level 6-level 8 Coverage\n",
    "    L6_coverage=test[test[target_level]>=6].label.value_counts()[0]/test[test.label==0].shape[0]\n",
    "    print(\"Level 6-Level 8: Loss is \",L6_loss[1], \"; Coverage is \",L6_coverage)\n",
    "    \n",
    "     #level 5-Leve 8 Loss (percentage of default people)\n",
    "    L5_loss = test[test[target_level]>=5 ].label.value_counts()/len(test[test[target_level]>=5 ])\n",
    "    #level 5- level 8 Coverage (percentage of good people)\n",
    "    L5_coverage=test[test[target_level]>=5 ].label.value_counts()[0]/test[test.label==0].shape[0]\n",
    "    print(\"Level 5-Level 8: Loss is \",L5_loss[1], \"; Coverage is \",L5_coverage)\n",
    "    \n",
    "    #level 4-level 8 Loss\n",
    "    L4_loss=test[test[target_level]>=4 ].label.value_counts()/len(test[test[target_level]>=4 ])\n",
    "    #level 4-level 8 Coverage\n",
    "    L4_coverage=test[test[target_level]>=4].label.value_counts()[0]/test[test.label==0].shape[0]\n",
    "    print(\"Level 4-Level 8: Loss is \",L4_loss[1], \"; Coverage is \",L4_coverage)\n",
    "    \n",
    "    \n",
    "    #level 3-level 8 Loss\n",
    "    L3_loss=test[test[target_level]>=3].label.value_counts()/len(test[test[target_level]>=3 ])\n",
    "    #level 3-level 8 Coverage\n",
    "    L3_coverage=test[test[target_level]>=3].label.value_counts()[0]/test[test.label==0].shape[0]\n",
    "    print(\"Level 3-Level 8: Loss is \",L3_loss[1], \"; Coverage is \",L3_coverage)\n",
    "    \n",
    "    #level 2-level 8 Loss\n",
    "    L2_loss=test[test[target_level]>=2].label.value_counts()/len(test[test[target_level]>=2 ])\n",
    "    #level 2-level 8 Coverage\n",
    "    L2_coverage=test[test[target_level]>=2].label.value_counts()[0]/test[test.label==0].shape[0]\n",
    "    print(\"Level 2-Level 8: Loss is \",L2_loss[1], \"; Coverage is \",L2_coverage)\n",
    "    \n",
    "    #level 1-level 8 Loss\n",
    "    L1_loss=test[test[target_level]>=1].label.value_counts()/len(test[test[target_level]>=1 ])\n",
    "    #level 1-level 8 Coverage\n",
    "    L1_coverage=test[test[target_level]>=1].label.value_counts()[0]/test[test.label==0].shape[0]\n",
    "    print(\"Level 1-Level 8: Loss is \",L1_loss[1], \"; Coverage is \",L1_coverage)\n",
    "    \n",
    "    #level 0-level 8 Loss\n",
    "    L0_loss=test[test[target_level]>=0].label.value_counts()/len(test[test[target_level]>=0 ])\n",
    "    #level 0-level 8 Coverage\n",
    "    L0_coverage=test[test[target_level]>=0].label.value_counts()[0]/test[test.label==0].shape[0]\n",
    "    print(\"Level 0-Level 8: Loss is \",L0_loss[1], \"; Coverage is \",L0_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_loss_coverage(test,target_level='CreditScore_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 7: save the scorcard\n",
    "data.to_csv('OrderScore_save7_whole_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card.predict(test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
